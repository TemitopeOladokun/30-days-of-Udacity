# 30-days-of-Udacity

## DAY 1 : 26/09/2019
1. I took the pledge #30daysofUdacity in my Deep Learning Nanodegree.
![](https://github.com/TemitopeOladokun/30-days-of-Udacity/blob/master/Screenshot%20(244).png)
2. Completed Lesson 1 on deep learning nanodegree class.
3. Revisited mlcourses.ai lesson on DEcision Trees and Random Forest.
https://www.youtube.com/watch?v=H4XlBTPv5rQ&feature=youtu.be
4. Watched Siraj Raval video on Decision Forest and Random Forest for clarity
https://www.youtube.com/watch?v=QHOazyP-YlM

## WHAT I LEARNT
* The collection of Decision Tree is Random Forest
* Random Forest used for both Classification and Regression problem
* Random Forest is good for small dataset.
* Broadened my knowledge on Matrix multiplication and Matrix dot product
* Decision tree is prone to error and it is instable



## DAY 2 : 27/09/2019
1. I started Introduction to Neural Networks
2. Broadened my knowledge on Perceptron    https://deepai.org/machine-learning-glossary-and-terms/perceptron
3. Watched a video on step function    https://www.youtube.com/watch?v=tHwpj9b4zZo

### WHAT I LEARNT
* The heart of deep learning is Neural Network 
* Neural networks have nodes, edges, layers
* Perceptron is an algorithm for binary classifier. It consist of foru main parts : Inptu values, Weights and biases, net sum and an activation function
* Another name for Perception is Linear Binary Classifier
* Default equation:
                    ##Equation = Wx + Bias
                    W = Weights
                    x = Values
                    B = Bias 



## DAY 3 : 28/09/2019
1. Read about the cost function equation
2. I learnt categories of model in Data Science
3. Different between local and global minimum   https://statinfer.com/204-5-10-local-vs-global-minimum/


### WHAT I LEARNT
* The co-efficient of a bias is 1
* If the question is to determine probability of a dataset. Use Predictive Model
* If the question is to show relationship of a dataset. Use Descriptive Model
* If the question requires Yes or No answer of a dataset. Use Classification Model
* Matrix is a rectangular array of numbers. Mathematically : Matrix = M x N
* Vector is a N x 1 matrix


## DAY 4: 29/09/2019
1. Lesson 1:12 - Non- Linear Region
2. Lesson 1:13 - Error Function
3. Lesson 1:14 - Softmax
4. Broadened my knowledge about softmax   https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax

### WHAT I LEARNT
* Error function is used to detect how close we are to the goal
* Error function must be differentiable
* Error function must be continuous bot discrete
* Predictions are answers gotten from Algorithm
* Converting step function to discrete is by using another activation function called Sigmoid function
* Softmax is another word for normalization of numbers
* To be proficint in AI, you need to be proficient in Object Oriented Programming


## DAY 5: 30/09/2019
1. Completed Lesson 1
2. Started Lesson 2
3. Explicit video of One-Hot Encoding  https://www.youtube.com/watch?v=v_4KWmkwmsU 
4. Watched this video image classifier https://www.youtube.com/watch?v=cAICT4Al5Ow




## DAY 6: 01/10/2019
1. Completed Lesson 2 - Implementing Gradient Descent
2. Started and Completed Lesson 3 - Training neural networks


                        
## DAY 7: 02/10/2019
1. Trying to Style Trasnfer my picture but not working. Trying to fix the error
2. Started Lesson 4 
3. Read a little about style Transfer


## DAY 8: 03/10/2019
1.	Completed all subtopics in Lesson 2 - Neural Networks 
2.	Viewed my first project â€“ Predicting Bike share pattern

### WHAT I LEARNT
* Difference between Underfitting and Overfitting
* What is Regularization? 
* L1 and L2 Regularization
* Dropout is the solution to Overfitting
* Random Restart is use to solve the problem of local minimum
* Other Activation function aside Sigmoid. They are Hyperbolic Tangent Function and Rectified Linear Unit (RELU) Function


## DAY 9: 04/10/2019
1. Working on my project - Predicting Bike-sharing Patterns
2. Trying to style transfer my images with the fast style transfer code 
3. Reading about how to build neural network with pytorch  
https://www.datahubbs.com/deep-learning-101-first-neural-network-with-pytorch/?fbclid=IwAR2_MxZ6aAgWKunezBjgCPn4mIE_tpflWLFZ0yMv-kBDsTD8KpwGlqRrYyU


### WHAT I LEARNT
* It is very interesting when you see the concepts of the class been implemented with codes.
* Learnt Forwardfeed Propagation and Back Propagation
* How to tune hyperparameters : Learning rate, number of iteractions et cetera


## DAY 10: 05/10/2019
1. Working on my project - Predicting Bike-sharing Patterns
2. Learning how to use Github professionally. Using https://www.udacity.com material 
3. Built multivariable using Linear regression


## DAY 11: 06/10/2019
1. Working on my project - Predicting Bike-sharing Patterns
2. Started Sentiment analysis videos


## DAY 12: 07/10/2019

1. Submitted my first project on Predicting Bike-sharing Patterns
2. Continued lessons on Sentiment analysis


## DAY 13:  08/10/2019
1.  I continued my course on sentiment analysis
2. Dived into some documentation in Python     https://docs.python.org/2/library/collections.html


## DAY 14: 09/10/2019
1. Solving the projects in Sentiment analysis videos
2. Studying some python documentation


## DAY 15: 10/10/2019
1. Solving the projects in Sentiment analysis videos


## DAY 16: 11/10/2019
1. Watched some videos on Neural Network
2. Read some medium article


## DAY 17: 12/10/2019
1. Attended a meetup where Linear regression and logistic regression were discussed
2. Learnt about the difference in their equations
3. Read some articles on neural network
4. I discovered there are some similarities between AI and Robotics

### WHAT I LEARNT
* I discovered that if linear equation is been used for logistic regression, it makes it difficult to identify the local minimum.


## DAY 18: 13/10/2019
1. Revisiting Bayes Rule in Introduction to Machine Learning Udacity's video
2. Read article about it   https://towardsdatascience.com/what-is-bayes-rule-bb6598d8a2fd


## DAY 19:14/10/2019
1. It is all about Algebraic mathematics
2. Worked on some Python code

### WHAT I LEARNT
* Impossibility is a Mirage - Brace up
* Brushin up on Python Object Oriented Programming


## DAY 20: 15/10/2019
1. Learning Algebraic concept. Thank you Udacity for an awesome material
2. Revisiting sentiment analysis
3. This article is enlightening    https://medium.com/dsnet/chai-time-data-science-show-announcement-bfaaf38df219
Tomorrow is another day for a good progress

### WHAT I LEARNT
* There is nothing as good has learning the basis
* Avoid assumptions


## DAY 21: 16/10/2019
1. Sorted out the error in my sentiment analysis code
2. Rounding up the lesson and moving on to Introduction to Deep Learning with Pytorch


## DAY 22: 17/10/2019
1. Going through Introduction to Deep Learning with Pytorch
2. Read about fully connected network  https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/ch04.html



## DAY 23:18/10/2019
1. Watching Introduction to Deep Learning with Pytorch videos
2. Going through the codes too


## DAY 24: 19/10/2019
1. Revised Sentiment analysis project


## DAY 25: 20/10/2019
1. Revising Backpropagation and also reading some articles


## DAY 26: 21/10/2019
1. Learning the basis has been an amazing experience - Essential mathematics topics in AI
2. Read some articles about AI and Deep Learning



## DAY 27: 22/10/2019
1. Clarified my doubt on Linear regression and logistic regression
https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html
2. Revising Introduction to deep learning with Pytorch
3. Watched a video about element-wise operation https://www.youtube.com/watch?v=2GPZlRVhQWY

### WHAT I LEARNT
* I also discovered that DREAMS are VALID but HARD-WORK AUTHENTICATE it.
* ![](https://github.com/TemitopeOladokun/30-days-of-Udacity/blob/master/Screenshot%20(296).png)
* ![](https://github.com/TemitopeOladokun/30-days-of-Udacity/blob/master/Screenshot%20(297).png)


## DAY 28: 23/10/2019
1. Solving Project 4 in Sentiment analysis
2. Solved Network Architecture with Pytorch


## DAY 29: 24/10/2019
1. Watched transfer learning videos
2. Solved some mathematics question
3. Read some articles about Covolution Neural Network
4. Read some articles about Recurrent Neural Network


## DAY 30: 25/10/2019
1. Started Convolutional Neural Network
2. Attended the Webinar by Sourena Yadegari (My Mentor)


## DAY 31: 26/10/2019
1. Continued with my CNN Videos
2. Read some CNN articles


## DAY 32: 27/10/2019
1. Read an article on MLP, CNN and its application with Pytorch


## DAY 33: 28/10/2019
1. Started the video on MLPs vs CNN
2. Learnt about Filters and Convolution layer
3. Filters and Edges
4. Frequency in Images as regards CNN

### WHAT I LEARNT
* MLPs convert images to Tensor while CNN convert to matrix
* CNN groups in edges
* Frequency in CNN is similar to that of sound wave
* MLPs are fully connected while CNN are sparsely connected. This leads to the network less dense


## DAY 34: 29/10/2019
1. Learnt about Pooling layers
2. Covolution layers in Pytorch
3. Image Augmentation
4. Visualizing CNN
5. CNNs in Pytorch
6. Augmentation using transfer learning

### WHAT I LEARNT 
* ReLU is mostly used in CNN because the slope of the graph doesnot saturate(approach zero). Stacking convolution layers requires a constant gradient instead of a diminished value. Leaky ReLU can also be used.
Parametric ReLU is a type of Leaky ReLU that figures out its slope itself.
https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7
https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6
* I discovered Concatenated ReLU was used here. Concatenated RELU is used to reduce redundancy that ReLU might have caused. It is denoted as C-ReLU     https://arxiv.org/pdf/1603.05201.pdf
* What is C-ReLU? https://arxiv.org/abs/1603.05201
* Increasing the depth of the convolution layer, it enables you to get more features
* Features becomes more subtle when the layers are increased
* There are different CNN Architecture that can be used for Transfer Learning: ImageNet, RestNet, VGG 16 


## DAY 35: 30/10/2019
1. Transfer Learning
2. Weight Initailization
3. Constant weight 
4. Normal Distribution and Random Uniform Distribution

### WHAT I LEARNT 
* Using constant weights make back propagation to fail becasue it is not design to deal with consistency
* Back propagation is designed to look at how different weight value affect training loss 
* Solution to a constant weight syndrome is choosing a random weight
* To get random weight, use uniform distribution
* Weight Initialization is about giving the model best chance to train


## DAY 36: 01/11/2019
1. Completed my videos on Weight Initialization

## DAY 37: 02/11/2019 
1. Working on my second project.


## DAY 38: 03/11/2019
1. Started watching videos on Autoencoder
2. Working on my second project

### WHAT I LEARNT
* Autoencoder is use to compress images without losing its content
 

## DAY 39: 04/11/2019 
1. Read some articles about CNN and RNN
2. Article on Pytorch and Neural Network
https://medium.com/dair-ai/pytorch-1-2-introduction-guide-f6fa9bb7597c

## DAY 40: 05/11/2019
1. Indepth knowledge abut Haar-code object detection in OpenCV 
https://docs.opencv.org/trunk/db/d28/tutorial_cascade_classifier.html
2. Open CV dataset
https://github.com/opencv/opencv/tree/master/data/haarcascades


## DAY 41: 06/11/2019
1. Gradient Descent for Machine Learning 
https://machinelearningmastery.com/gradient-descent-for-machine-learning/
2. More information about activation functions
http://cs231n.github.io/neural-networks-1/#actfun
https://www.datasciencecentral.com/profiles/blogs/deep-learning-advantages-of-relu-over-sigmoid-function-in-deep
3. First research paper on Dropout
https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf
4. Watching a video on Numpy 
https://youtu.be/QUT1VHiLmmI

### WHAT I LEARNT
* Stochastic gradient descent is been used when we have large dataset. 
* Sigmoid function has been mostly used but recently there is a shift in the usage because of its drawback.
* The two drawbacks for sigmoid are: (1) Sigmoid saturates and kills gradient descent (2) The output is not zero centred
* Tanh is a scaled sigmoid neuron
* Tanh non-linearity is mostly preferred to Sigmoid non linearity
* Numpy is faster than List 


## DAY 42:07/11/2019
1. Working on my project 

### What I Learnt
* It might take longer than I expected
